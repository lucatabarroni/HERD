{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d0ee96-081a-42a5-973a-7fa451a406b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:27:35.270705: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 17:27:40.213628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-02 17:27:40.269316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-02 17:27:40.269658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21a6f94-6f4a-4cd7-86a8-b05ee79d66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Model,model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from itertools import repeat\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import uproot as up\n",
    "import pickle\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32b6912-bdeb-4ddf-833c-1e36fa987ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher():\n",
    "    def __init__(self,size_array,names_array,batch_size=32):\n",
    "        \n",
    "        self.size_array=size_array\n",
    "        self.names_array=names_array\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "\n",
    "\n",
    "    def batches_creation(self):\n",
    "        ev_nel_batch=0\n",
    "        num_batch=0\n",
    "        num_file=0\n",
    "        ev_da_file=0\n",
    "        ev_to_skip=0\n",
    "        final_batch_origin=[[]]\n",
    "        ev_nel_file=self.size_array[num_file]\n",
    "        for i in range(sum(self.size_array)):\n",
    "            ev_nel_batch+=1\n",
    "            ev_nel_file-=1\n",
    "            ev_da_file+=1\n",
    "            if ev_nel_batch==self.batch_size:\n",
    "                final_batch_origin[num_batch].append([self.names_array[num_file],ev_to_skip,ev_to_skip+ev_da_file])\n",
    "                ev_to_skip+=ev_da_file\n",
    "                ev_da_file=0\n",
    "                num_batch+=1\n",
    "                ev_nel_batch=0\n",
    "                final_batch_origin.append([])\n",
    "            if ev_nel_file==0:\n",
    "                final_batch_origin[num_batch].append([self.names_array[num_file],ev_to_skip,ev_to_skip+ev_da_file])\n",
    "                num_file+=1\n",
    "                if num_file==len(self.size_array):\n",
    "                    ev_nel_batch=0\n",
    "                    num_batch=0\n",
    "                    num_file=0\n",
    "                    ev_da_file=0\n",
    "                    ev_to_skip=0\n",
    "                    break\n",
    "                ev_nel_file=self.size_array[num_file]\n",
    "                ev_to_skip=0\n",
    "                ev_da_file=0\n",
    "        return final_batch_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23fe528c-c795-4f36-a69d-4e5b99f0463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Batcher,Sequence):\n",
    "    def __init__(self,\n",
    "                 list_low_elcectrons,list_high_electrons,list_low_protons,list_high_protons,\n",
    "                 list_low_electrons_size,list_high_electrons_size,list_low_protons_size,list_high_protons_size,\n",
    "                 massimo,batch_size=64):\n",
    "        \n",
    "        self.low_electrons=list_low_elcectrons\n",
    "        self.high_electrons=list_high_electrons\n",
    "        self.low_protons=list_low_protons\n",
    "        self.high_protons=list_high_protons\n",
    "\n",
    "        self.low_electrons_size=list_low_electrons_size\n",
    "        self.high_electrons_size=list_high_electrons_size\n",
    "        self.low_protons_size=list_low_protons_size\n",
    "        self.high_protons_size=list_high_protons_size\n",
    "\n",
    "        self.massimo=massimo\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.low_ele_labels=[]\n",
    "        self.high_ele_labels=[]\n",
    "        self.low_pro_labels=[]\n",
    "        self.high_pro_labels=[]\n",
    "\n",
    "        self.dataset=pd.DataFrame()\n",
    "        self.labels=[[]]\n",
    "\n",
    "        self.perc_low_electrons=sum(self.low_electrons_size)/(sum(self.low_electrons_size)+sum(self.high_electrons_size)+sum(self.low_protons_size)+sum(self.high_protons_size))\n",
    "        self.perc_high_electrons=sum(self.high_electrons_size)/(sum(self.low_electrons_size)+sum(self.high_electrons_size)+sum(self.low_protons_size)+sum(self.high_protons_size))\n",
    "        self.perc_high_protons=sum(self.high_protons_size)/(sum(self.low_electrons_size)+sum(self.high_electrons_size)+sum(self.low_protons_size)+sum(self.high_protons_size))\n",
    "        self.perc_low_protons=sum(self.low_protons_size)/(sum(self.low_electrons_size)+sum(self.high_electrons_size)+sum(self.low_protons_size)+sum(self.high_protons_size))\n",
    "\n",
    "        self.low_electrons_per_batch=int(self.batch_size*self.perc_low_electrons)\n",
    "        self.high_electrons_per_batch=int(self.batch_size*self.perc_high_electrons)\n",
    "        self.low_protons_per_batch=int(self.batch_size*self.perc_low_protons)\n",
    "        self.high_protons_per_batch=self.batch_size-self.low_electrons_per_batch-self.high_electrons_per_batch-self.low_protons_per_batch\n",
    "        \n",
    "        self.low_electrons_batch_per_file=Batcher(self.low_electrons_size,self.low_electrons,self.low_electrons_per_batch).batches_creation()\n",
    "        self.high_electrons_batch_per_file=Batcher(self.high_electrons_size,self.high_electrons,self.high_electrons_per_batch).batches_creation()\n",
    "        self.low_protons_batch_per_file=Batcher(self.low_protons_size,self.low_protons,self.low_protons_per_batch).batches_creation()\n",
    "        self.high_protons_batch_per_file=Batcher(self.high_protons_size,self.high_protons,self.high_protons_per_batch).batches_creation()\n",
    "           \n",
    "        self.min_len=min(len(self.low_electrons_batch_per_file),len(self.high_electrons_batch_per_file),len(self.low_protons_batch_per_file),len(self.high_protons_batch_per_file))-1\n",
    "\n",
    "        self.low_electrons_batch_per_file=self.low_electrons_batch_per_file[:self.min_len]\n",
    "        self.high_electrons_batch_per_file=self.high_electrons_batch_per_file[:self.min_len]\n",
    "        self.low_protons_batch_per_file=self.low_protons_batch_per_file[:self.min_len]\n",
    "        self.high_protons_batch_per_file=self.high_protons_batch_per_file[:self.min_len]\n",
    "        \n",
    "        low_ele_ev_last_cell=0\n",
    "        for low_ele_last_cell in self.low_electrons_batch_per_file[-1]:\n",
    "            low_ele_ev_last_cell+=low_ele_last_cell[2]-low_ele_last_cell[1]\n",
    "        \n",
    "        high_ele_ev_last_cell=0\n",
    "        for high_ele_last_cell in self.high_electrons_batch_per_file[-1]:\n",
    "            high_ele_ev_last_cell+=high_ele_last_cell[2]-high_ele_last_cell[1]\n",
    "        \n",
    "        low_pro_ev_last_cell=0\n",
    "        for low_pro_last_cell in self.low_protons_batch_per_file[-1]:\n",
    "            low_pro_ev_last_cell+=low_pro_last_cell[2]-low_pro_last_cell[1]\n",
    "        \n",
    "        high_pro_ev_last_cell=0\n",
    "        for high_pro_last_cell in self.high_protons_batch_per_file[-1]:\n",
    "            high_pro_ev_last_cell+=high_pro_last_cell[2]-high_pro_last_cell[1]\n",
    "        \n",
    "        for i in range(self.min_len):\n",
    "            self.low_ele_labels=([1]*self.low_electrons_per_batch)\n",
    "            self.high_ele_labels=([1]*self.high_electrons_per_batch)\n",
    "            self.low_pro_labels=([0]*self.low_protons_per_batch)\n",
    "            self.high_pro_labels=([0]*self.high_protons_per_batch)\n",
    "            self.labels[i]=(self.low_ele_labels+self.high_ele_labels+self.low_pro_labels+self.high_pro_labels)\n",
    "            if i==self.min_len-1:\n",
    "                break\n",
    "            else:\n",
    "                self.labels.append([])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.min_len\n",
    "    \n",
    "    def creation_dataset(self,idx):\n",
    "        matr_batch=[]\n",
    "        for i in range(int(self.dataset.shape[0]/400)):\n",
    "            matrix = np.array(self.dataset.iloc[i*400:(i+1)*400]).reshape(20, 20)\n",
    "            matrix=matrix/self.massimo\n",
    "            matr_batch.append(matrix)\n",
    "\n",
    "        shuffled=list(zip(matr_batch,self.labels[idx]))\n",
    "        random.shuffle(shuffled)\n",
    "\n",
    "        self.dataset=pd.DataFrame()\n",
    "\n",
    "        inputs,targets=zip(*shuffled)\n",
    "        inputs=np.array(inputs)\n",
    "        targets=np.array(targets)\n",
    "\n",
    "        return inputs,targets\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        for low_ele_events in self.low_electrons_batch_per_file[idx]:\n",
    "            with up.open(low_ele_events[0][:-14]) as file:\n",
    "                low_ele_df = file['showersTree'].arrays('deps2D', library='pd')[low_ele_events[1]*400:low_ele_events[2]*400]\n",
    "                self.dataset=pd.concat([self.dataset,low_ele_df])\n",
    "        \n",
    "        for high_ele_events in self.high_electrons_batch_per_file[idx]:\n",
    "            with up.open(high_ele_events[0][:-14]) as file:\n",
    "                high_ele_df = file['showersTree'].arrays('deps2D', library='pd')[high_ele_events[1]*400:high_ele_events[2]*400]\n",
    "                self.dataset=pd.concat([self.dataset,high_ele_df])\n",
    "                \n",
    "        for low_pro_events in self.low_protons_batch_per_file[idx]:\n",
    "            with up.open(low_pro_events[0][:-14]) as file:\n",
    "                low_pro_df = file['showersTree'].arrays('deps2D', library='pd')[low_pro_events[1]*400:low_pro_events[2]*400]\n",
    "                self.dataset=pd.concat([self.dataset,low_pro_df])\n",
    "            \n",
    "        for high_pro_events in self.high_protons_batch_per_file[idx]:\n",
    "            with up.open(high_pro_events[0][:-14]) as file:\n",
    "                high_pro_df = file['showersTree'].arrays('deps2D', library='pd')[high_pro_events[1]*400:high_pro_events[2]*400]\n",
    "                self.dataset=pd.concat([self.dataset,high_pro_df])\n",
    "\n",
    "        return self.creation_dataset(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415d7c4a-8677-4c71-aae2-3847e4e98ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_information/electrons_100GeV_1TeV_path_size','rb') as f:\n",
    "    electrons_100GeV_1TeV_path_size=pickle.load(f)\n",
    "    \n",
    "with open('Dataset_information/electrons_1TeV_20TeV_path_size','rb') as f:\n",
    "    electrons_1TeV_20TeV_path_size=pickle.load(f)\n",
    "\n",
    "with open('Dataset_information/protons_100GeV_1TeV_path_size','rb') as f:\n",
    "    protons_100GeV_1TeV_path_size=pickle.load(f)\n",
    "\n",
    "with open('Dataset_information/protons_1TeV_10TeV_path_size','rb') as f:\n",
    "    protons_1TeV_10TeV_path_size=pickle.load(f)\n",
    "\n",
    "electrons_100GeV_1TeV_path=[row[0] for row in electrons_100GeV_1TeV_path_size]\n",
    "electrons_1TeV_20TeV_path=[row[0] for row in electrons_1TeV_20TeV_path_size]\n",
    "\n",
    "protons_100GeV_1TeV_path=[row[0] for row in protons_100GeV_1TeV_path_size]\n",
    "protons_1TeV_10TeV_path=[row[0] for row in protons_1TeV_10TeV_path_size]\n",
    "\n",
    "electrons_100GeV_1TeV_size=[row[1] for row in electrons_100GeV_1TeV_path_size]\n",
    "electrons_1TeV_20TeV_size=[row[1] for row in electrons_1TeV_20TeV_path_size]\n",
    "\n",
    "protons_100GeV_1TeV_size=[row[1] for row in protons_100GeV_1TeV_path_size]\n",
    "protons_1TeV_10TeV_size=[row[1] for row in protons_1TeV_10TeV_path_size]\n",
    "\n",
    "electrons_100GeV_1TeV_training_size=sum(electrons_100GeV_1TeV_size)/2\n",
    "electrons_1TeV_20TeV_training_size=sum(electrons_1TeV_20TeV_size)/2\n",
    "\n",
    "protons_100GeV_1TeV_training_size=sum(protons_100GeV_1TeV_size)/2\n",
    "protons_1TeV_10TeV_training_size=sum(protons_1TeV_10TeV_size)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48d5c80-abb2-48ca-a530-82f0b856b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "somma=0\n",
    "for i in range(len(electrons_100GeV_1TeV_size)):\n",
    "    somma+=electrons_100GeV_1TeV_size[i]\n",
    "    if somma > electrons_100GeV_1TeV_training_size:\n",
    "        final_electrons_100GeV_1TeV_training_file=i+1\n",
    "        break\n",
    "\n",
    "somma=0\n",
    "for i in range(len(electrons_1TeV_20TeV_size)):\n",
    "    somma+=electrons_1TeV_20TeV_size[i]\n",
    "    if somma > electrons_1TeV_20TeV_training_size:\n",
    "        final_electrons_1TeV_20TeV_training_file=i+1\n",
    "        break\n",
    "\n",
    "\n",
    "somma=0\n",
    "for i in range(len(protons_100GeV_1TeV_size)):\n",
    "    somma+=protons_100GeV_1TeV_size[i]\n",
    "    if somma > protons_100GeV_1TeV_training_size:\n",
    "        final_protons_100GeV_1TeV_training_file=i+1\n",
    "        break\n",
    "\n",
    "somma=0\n",
    "for i in range(len(protons_1TeV_10TeV_size)):\n",
    "    somma+=protons_1TeV_10TeV_size[i]\n",
    "    if somma > protons_1TeV_10TeV_training_size:\n",
    "        final_protons_1TeV_10TeV_training_file=i+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223c40a6-aaa5-4fe1-900d-9297ff1b7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons_100GeV_1TeV_path_tree=[name+':showersTree;1' for name in electrons_100GeV_1TeV_path]\n",
    "electrons_1TeV_20TeV_path_tree=[name+':showersTree;1' for name in electrons_1TeV_20TeV_path]\n",
    "\n",
    "protons_100GeV_1TeV_path_tree=[name+':showersTree;1' for name in protons_100GeV_1TeV_path]\n",
    "protons_1TeV_10TeV_path_tree=[name+':showersTree;1' for name in protons_1TeV_10TeV_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2d112e-84d5-4c6f-b436-1df8e7296cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_information/max_training_values','rb') as f:\n",
    "    massimo=pickle.load(f)\n",
    "\n",
    "with open('Dataset_information/min_training_values','rb') as f:\n",
    "    minimo=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fd96e0-56ba-486e-9444-c6bce52bb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_path_tree=electrons_100GeV_1TeV_path_tree[:final_electrons_100GeV_1TeV_training_file]+electrons_1TeV_20TeV_path_tree[:final_electrons_1TeV_20TeV_training_file]+protons_100GeV_1TeV_path_tree[:final_protons_100GeV_1TeV_training_file]+protons_1TeV_10TeV_path_tree[:final_protons_1TeV_10TeV_training_file]\n",
    "\n",
    "list_electrons_training_size=electrons_100GeV_1TeV_size[:final_electrons_100GeV_1TeV_training_file]+electrons_1TeV_20TeV_size[:final_electrons_1TeV_20TeV_training_file]\n",
    "\n",
    "list_protons_training_size=protons_100GeV_1TeV_size[:final_protons_100GeV_1TeV_training_file]+protons_1TeV_10TeV_size[:final_protons_1TeV_10TeV_training_file]\n",
    "\n",
    "list_training_size=list_electrons_training_size+list_protons_training_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d47e0a4-948f-4647-879b-ee85d1862fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    decay_rate = 0.94\n",
    "    if epoch % 2 == 0 and epoch != 0:\n",
    "        return lr * decay_rate\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "checkpoint = ModelCheckpoint('model_trained/CNN-{epoch:03d}-'+str(batch_size)+'.h5', verbose=1, save_best_only=False, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf9507-8a51-4183-a1fd-f1f349ceec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ResNet_model_pre-trained/ResNet_CNN.json', 'r') as f:\n",
    "    modello_json = f.read()    \n",
    "model= model_from_json(modello_json)\n",
    "model.compile(optimizer=keras.optimizers.SGD(lr=0.045, momentum=0.9), loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcc8039e-4828-439d-aeec-9e7cb9171e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "object=DataLoader(electrons_100GeV_1TeV_path_tree[:final_electrons_100GeV_1TeV_training_file],electrons_1TeV_20TeV_path_tree[:final_electrons_1TeV_20TeV_training_file]\n",
    "                   ,protons_100GeV_1TeV_path_tree[:final_protons_100GeV_1TeV_training_file],protons_1TeV_10TeV_path_tree[:final_protons_1TeV_10TeV_training_file],\n",
    "                   electrons_100GeV_1TeV_size[:final_electrons_100GeV_1TeV_training_file],electrons_1TeV_20TeV_size[:final_electrons_1TeV_20TeV_training_file],\n",
    "                   protons_100GeV_1TeV_size[:final_protons_100GeV_1TeV_training_file],protons_1TeV_10TeV_size[:final_protons_1TeV_10TeV_training_file],massimo,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d422696-93ad-4346-9e22-deb0f19f0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(object,epochs=50,shuffle=True,callbacks=[callback,checkpoint],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c37e3-149a-4cc0-a351-25e0e4574aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HERD",
   "language": "python",
   "name": "hep2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
